7.17 更好的优化和正则化
	 近期深度学习应用到语音识别声学模型取得重大进步的另一个领域是优化准则和方法，及其相关的避免深度网络过拟合的正则化技术。
	 一项早期在语音识别DNN的研究，其由Microsoft Research进行，并在[260]中报告，第一次认识到传统DNN训练过程中所需错误率和交叉熵训练准则的不匹配问题。解决方法是使用基于全序列的最大互信息为优化目标代替帧级的交叉熵训练准则，在和HMM结合的浅层神经网络中也使用同样的方法定义训练目标。同样的，这等价于在DNN的顶层加上条件随机场(CRF)，代替原有DNN中的softmax层（注意这篇论文中将DNN称为DBN）。这个新的序列化判别式学习的技术也发展使用到优化DNN权值，CRF转移权值和二音素的语言模型。这里要注意的是，该语音任务数据集为TIMIT，使用一个简单二元音素的类语言模型。二元语言模型简单性在于它允许全序列的训练，不需要词网络，大幅度降低了训练的复杂度。
	 另一个触发全序列训练方法的是[260]，早期的DNN音素识别任务在静态模式分类，交叉熵去优化DNN权值是均使用标准基于帧的目标函数。HMM中状态转移参数和语言模型的训练与DNN权值训练独立。但是，众所周知，在HMM研究历史中，序列化分类准则对提高语音识别和音素识别率非常有帮助，因为序列化分类准则与性能评测方法（例如音素或词错误率）比帧级交叉熵准则更直接相关。更准确的说，使用帧级交叉熵准则训练音素序列识别的DNN没有显示考虑在给帧分配音素标注概率分布时相邻帧之间距离更小。为了克服这个缺点，在给定全部句子可见特征或等价的DNN提取的隐层特征序列时，可以优化标注序列之间的条件概率。为了优化训练数据对数域的条件概率，反向梯度可以由激活函数参数，转移参数和低层的网络权值获得，然后在句子级进行误差反向传播算法。我们注意到在更早的研究[212]，文中结合CRF类似结构和神经网络，其数学形式含CRF的特殊形式。还有，[194,291]中更早的提到了在浅层神经网络中使用全序列的分类准则。
	 在实现[260]中描述的上述DNN系统的全序列学习算法时，DNN网络权值使用帧级的交叉熵初始花。转移概率结合HMM转移矩阵和二元音素语言模型得分初始化，并在共同优化前通过固定DNN权值调节转移矩阵参数得到进一步优化。使用仔细的调度的共同优化减少过拟合，结果显示使用全序列优化训练比帧级训练的DNN性能相对提高5%[260]。没有减少过拟合时，MMI准则训练的DNN比帧级交叉熵准则的更容易陷入过拟合。这是因为训练集，开发集和测试集数据帧级之间的关系可能不同。更重要的是，这中不同在使用帧级目标函数训练时并未出现。
	 对大规模DNN-HMM系统而言，使用帧级或全序列的优化目标，利用大模型的大量训练数据加速训练十分必要。除了上述的方法，Dean的[69]提出了在大词汇量语音识别中使用异步的随机梯度下降[ASGD]方法，自适应的梯度下降(Adgrad)，和大规模受限存储BFGS(L-BFGS)方法。Sainath的[312]对一系列加速训练基于DNN语音识别系统的优化方法。
