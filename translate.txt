7.1.4 基于DNN识别器的自适应
	 DNN-HMM是90年代人工智能和HMM混合系统的升级版本，这期间发展了很多自适应的技术，其中大部分基于对输入层或输出层的网络权值做线性变换。许多基于DNN的自适应探索研究也同样基于上述的线性变换方法[223, 401, 402]。然而，与早期的窄层和浅层网络相比，由于更深更宽的隐层结构和更大的上下文依赖的因素和状态输出，DNN-HMM的参数显著增大。这种不同带来了DNN-HMM系统自适应的新挑战，尤其是自适应数据较少的情况。这里我们讨论在大规模DNN系统下克服这些挑战的的最近几种代表性的研究。
	 Yu的[430]提出了DNN正规化自适应技术。它强制由自适应之后模型分布估计自适应之前的模型尽可能接近，适当的修正权值。这个约束通过对自适应规则增加Kullback–Leibler（KLD）正则化实现。这种正则化与通过传统误差反向传播算法修正目标分布等价，因此训练的DNN模型大部分参数仍未改变。新的目标分布由自适应之前的模型插值和真实数据的对齐生成。这种插值通过防止自适应模型远离说话人独立模型，从而避免过度训练。这种正则化的自适应方法与L2正则化不同，L2限制模型参数本身而非输出概率。
	在[330]，DNN自适应并非使用传统网络权值而是隐层激活函数。这种方法有效的克服了现有基于线性变换自适应方法依赖于输入或输出层的限制，因为新的方法仅需要对一定数量的隐层激活函数自适应。
	最近，Saon的[317]探索一种语音识别自适应十分有效的新方法。这种方法组合I-vectors特征和fMLLR特征作为DNN的输入。I-vectors(Identity vectors)常常用于说话人验证和语音识别应用，因为该方法可以降说话人相关信息压缩为低维特征的特性。而fMLLR是GMM-HMM系统自适应的一种非常有效的技术。因为I-vectors不服从频率的局部性，因此必须仔细的与服从频率局部性的fMLLR特征组合。多尺度的CNN-DNN系统拥有组合不同类型特征的特性。在解码阶段和训练阶段，特定说话人的I-vectors特征都附加到帧级的fMLLR特征之后。
