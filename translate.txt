	RNN的一种特殊形式是储藏模型或输出状态网络，普通RNN中的输出层非线性单元改为固定的线性单元，权值矩阵是精心设计而非训练学习所得。输入矩阵也是固定的，非学习而来，部分因为参数学习的困难性。只有隐层和输出层的权值矩阵是通过学习而来。由于输出是线性的，全局优化由封闭形式的解，所以参数学习非常高效。但是因为许多参数并未学习，所以隐层必须足够大才能获得好的结果。Triefenbach的[365]将这种模型应用到音素识别，获得适当好的识别精度。
	Palangi的[276]提出了储藏模型的改进版，通过前一个利用线性输出，固定输入矩阵和循环矩阵获得的模型，学习输入矩阵和循环矩阵，从而简化输出矩阵的学习。而且，提出了一个利用储藏模型线性输出学习输入矩阵和循环矩阵的特殊技术。与训练一般RNN的时间误差反向传播算法（BPTT）相比，这个提出的技术给利用线性输出单元特性给RNN中不同的矩阵增加了限制，替换BPTT递归梯度，使得梯度计算和学习信号有可分析的形式。
	除了上面的最近用于语音识别的深度学习模型上的几个创新之外，也有不断增长的在更好的非线性单元设计和实现工作。尽管sigmoid和tanh是DNN最常用的非线性单元，但它们的缺点也很明显。例如，当网络单元在不同方向都接近饱和时，梯度变化小，整个网络的学习变得很慢。Jaitly和Hinton[183]为了克服sigmoid单元的缺点，最先在DNN语音识别中使用改进的线性单元(ReLU)。ReLU是指在网络中使用形如f(x) = max(0, x)的激活函数。Dahl的[65]和Mass的[234]成功在大词汇量语音识别上应用ReLU，最好的识别精度在结合ReLU和正则化技术dropout时获得。
	最近提出的另一种在语音识别上有用的DNN单元是"maxout"单元，它用于构建深度的"maxout"网络，如[244]所述。一个深度"maxout"网络由多层以maxout为激活函数的单元组成，在一组固定输入权值上进行最大化或maxout操作。这与之前讨论的语音识别和计算机视觉中的max pooling类似，每一组最大值由前一层得到。更近的，Zhang的[441]中将maxout单元分为两类，第一种"soft-maxout"将原来的最大化操作替换为soft-max函数；第二种，p-norm单元，使用非线性的××××。实验说明p-norm单元使用***比maxout，tanh和ReLU单元效果均好。Gulcehre的[138]中提出和研究了p-norm自动学习。
	最后，Srivastava的[350]中提出另一类新的非线性单元，称作winner-take-all单元。他将临近的神经元之间的竞争纳入前向网络结构，之后使用不同的后向算法训练。Winner-take-all是一种非常有趣的非线性单元的形式，它建立神经元组(通常为2个)，在一组之中的除过最大值的神经元其他所有神经元都为0值。实验表明使用这种非线性单元的网络比标准的sigmoid具有记忆性。这种新型非线性单元还有待于在语音识别任务上评测。
