	除了上述侧重于优化和完全监督的范例，即训练数据均有标注，也有研究DNN-HMM语音识别系统的半监督的训练。Liao[223]报告了一项非常有挑战的研究，它在YouTube Speech上使用半监督训练DNN-HMM。其主要技术是使用“island of confidence”启发式的过滤选择训练片段。另外，Vesely[374]也探索了DNN的半监督训练，使用自训练作为句子级和帧级置信选择的基本策略。由混淆网络生成的每帧的置信度进行帧级选择。Huang[176]报告了半监督训练的另一个变化，其中使用多系统的组合和信度重估选择训练数据。此外，Thomas[362]克服了在一系列低资源情景中缺乏声学模型所需的训练数据问题。他们首先准备转录的多语言数据和半监督训练，并使用他们提出的前端特征，随后建立语音识别系统。
	最后，我们看到了由Hinton等[166]提出的新正则化方法"dropout"在基于深度学习的语音识别取得的巨大进步。过拟合在DNN训练中很容易出现，DNN的多层激励也易于相互适应去拟合输入声学数据。Dropout是限制相互适应的技术。它的具体操作如下。每个训练实例，每个隐层单元都随机的以一定概率（如××）被忽略，随后除了简单的缩放DNN权重外(因子**)，解码正常完成。或者，DNN权值的缩放可以在训练阶段完成[缩放因子××]。Dropout正则化的好处是，训练DNN的过程是使隐层单元仅受自身激励影响，而不依赖其他的单元，并提供了一种在不同网络中求其平均模型的方法。这些优点在训练数据有限时最为明显，或者当DNN网络大小比训练数据要大的多时。Dahl[65]在使用ReLU单元的全连接的DNN的一些高层中应用共轭dropout。Seltzer和Yu[325]将dropout应用到噪声鲁棒的语音识别。Deng[81]从另一方面入手，将dropout应用到卷积神经网络的所有层，包括高层的全连接层，低层局部连接的卷积层和pooling层，并发现在卷积神经网络中dropout参数值需要大幅降低。
	随后dropout的应用包括Miao和Metze[243]，是在低资源数据稀疏条件下使用基于DNN的语音识别。最近，Sainath[306]将dropout和一些新的技巧结合（在本节中有描述，包括使用深度CNN，Hessian-free序列化学习，使用ReLU单元，使用fMLLR和滤波器组特征等），并在一系列大词汇量语音识别任务上获得了最领先的结果。
	作为总结，2010年报告的应用深度学习方法到语音分析和识别领域的成功，在过去三年中取得了长足的发展。我们看到在这个主题上的研究工作和刊物爆炸性增长，看到它在语音识别社区激动人心的成功。我们希望基于深度学习的语音识别研究能够继续壮大，至少在不远的将来能继续成长。公平坦白的说，基于深度学习的大规模语音识别的不断成功（如本章开头所述，到ASRU-2013截止）是使用深度学习方法到其他领域研究和应用的关键刺激，我们会接着在8-11章继续讨论。
