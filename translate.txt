	也许循环神经网络和其堆叠或深度版是其中最著名的深度结构[135, 136, 153, 279, 377]。尽管RNN最早在音素识别[304]中取得成功，但由于其训练的错综复杂性，很难轻易复制，更不用说应用在大规模的语音识别任务了。从此之后，RNN的学习算法得到很大的提升，最近的RNN的使用也获得了更好的结果，特别是双向的LSTM(long short-term memory)的使用。双向RNN的信息流和LSTM的基本单元分别如图7.3和7.4所示。
	众所周知，优于梯度弥散或者爆炸[280]，学习RNN的参数十分困难。Chen和Deng的[48]，Deng和Chen的[85]开发了一种原始-对偶的的训练方法，它将RNN的学习问题抽象为标准的优化问题，通过最大化交叉熵，限制RNN的循环矩阵小于固定的值，从而保证动态RNN的稳定性。在音素识别的实验结果如下：(1) 原始-对偶技术对训练RNN非常有效，在限制梯度上优于早期的启发式方法。(2) 使用DNN计算的高层语音特征作为RNN的输入相比没有使用DNN的识别，其识别精度更高。(3) 当逐次使用DNN从高层低层的特征时，识别精度逐渐下降。
	图7.3 双向RNN的信息流，给出了图模型和公式。W是权值矩阵，图中没有标注出来，但很容易推测到[136,@IEEE]
	图7.4 LSTM单元的信息流，给出了图模型和公式。W是权值矩阵，图中没有标注出来，但很容易推测到[136,@IEEE]
